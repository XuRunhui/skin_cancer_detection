{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import h5py\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import resample\n",
    "from torchvision.models import efficientnet_b0, efficientnet_b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, hdf5_file, transform=None, mode='train', train_data=None):\n",
    "        # self.img_dir = img_dir\n",
    "        self.mode = mode\n",
    "        if self.mode == \"train\":\n",
    "            print(\"train data init begin\")\n",
    "        elif self.mode == \"val\":\n",
    "            print(\"val data init begin\")\n",
    "\n",
    "        self.transform = transform\n",
    "        self.hdf5_file = h5py.File(hdf5_file, 'r')\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "        self.categorical_vars = ['sex', 'anatom_site_general',\n",
    "                             'image_type', 'tbp_tile_type', 'tbp_lv_location', \n",
    "                             'tbp_lv_location_simple', 'attribution', 'copyright_license', \n",
    "                             'lesion_id', 'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', \n",
    "                             'iddx_4', 'iddx_5', 'mel_mitotic_index']\n",
    "        \n",
    "        self.numerical_vars = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', \n",
    "                          'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', \n",
    "                          'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n",
    "                          'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', \n",
    "                          'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', \n",
    "                          'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', \n",
    "                          'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', \n",
    "                          'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', \n",
    "                          'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', \n",
    "                          'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', \n",
    "                          'tbp_lv_y', 'tbp_lv_z', 'tbp_lv_dnn_lesion_confidence']\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.annotations = self.get_full_dataframe(csv_file)\n",
    "            self.encode_labels()\n",
    "            self.normalize_numerical_data()\n",
    "        elif mode == 'val' or mode == 'test' and train_data is not None:\n",
    "            print(\"getting full dataframe\")\n",
    "            self.annotations = self.get_full_dataframe(csv_file, train_data=train_data)\n",
    "            print(\"encoding labels\")\n",
    "            self.encode_labels(train_data)\n",
    "            print(\"normalizing numerical data\")\n",
    "            self.normalize_numerical_data(train_data)\n",
    "\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            print(\"train data init done\")\n",
    "        elif self.mode == \"val\":\n",
    "            print(\"val data init done\")\n",
    "        # print(numerical_df.isnull().sum())\n",
    "        # print(categorical_df.isnull().sum())\n",
    "        # print(numerical_df.values.dtype)\n",
    "        # print(categorical_df.values.dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        # img_path = os.path.join(self.img_dir, self.annotations.iloc[idx][\"isic_id\"] + \".jpg\")\n",
    "        # image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        \n",
    "        isic_id = self.annotations.iloc[idx][\"isic_id\"]\n",
    "\n",
    "        if isic_id in self.hdf5_file:\n",
    "            image = self.hdf5_file[isic_id]\n",
    "            # Check if the data is numerical before conversion\n",
    "            image_data = image[()]\n",
    "            # 将字节字符串解码为图像\n",
    "            image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "                \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load categorical data\n",
    "\n",
    "        numerical_df = self.annotations[self.numerical_vars]\n",
    "        categorical_df = self.annotations[self.categorical_vars]\n",
    "        numerical_df = numerical_df.iloc[idx]\n",
    "        categorical_df = categorical_df.iloc[idx]\n",
    "\n",
    "        numerical_data = torch.tensor(numerical_df.values, dtype=torch.float)\n",
    "        categorical_data = torch.tensor(categorical_df.values, dtype=torch.long)\n",
    "\n",
    "        # Load numerical data\n",
    "        if self.mode in ['train', 'val']:\n",
    "            label = torch.tensor(int(self.annotations.iloc[idx][\"target\"]))\n",
    "            return image, categorical_data, numerical_data, label\n",
    "\n",
    "        elif self.mode == 'test':\n",
    "            isic_id = self.annotations.iloc[idx][\"isic_id\"]\n",
    "            return image, categorical_data, numerical_data, isic_id\n",
    "        \n",
    "\n",
    "    def get_full_dataframe(self, df, train_data=None):\n",
    "        def fill_missing_with_distribution(series, distribution):\n",
    "            missing_indices = series[series.isna()].index\n",
    "            filled_values = np.random.choice(distribution.index, size=len(missing_indices), p=distribution.values)\n",
    "            series.loc[missing_indices] = filled_values\n",
    "            return series\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            df['lesion_id'] = df['lesion_id'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "        elif self.mode != 'train':\n",
    "            for category in ['lesion_id', 'mel_mitotic_index']:\n",
    "                dis = train_data.annotations[category].value_counts(normalize=True)\n",
    "                generated_lesion_ids = np.random.choice(dis.index, size=len(df), p=dis.values)\n",
    "                df[category] = generated_lesion_ids\n",
    "\n",
    "        for category in ['sex', 'anatom_site_general']:\n",
    "            dis = df[category].value_counts(normalize=True)\n",
    "            df[category] = fill_missing_with_distribution(df[category], dis)\n",
    "        \n",
    "        mean_age = df['age_approx'].mean()\n",
    "        df['age_approx'] = df['age_approx'].fillna(mean_age)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            df['iddx_2'] = df['iddx_2'].fillna(df['iddx_1'])\n",
    "            df['iddx_3'] = df['iddx_3'].fillna(df['iddx_2'])\n",
    "            df['iddx_4'] = df['iddx_4'].fillna(df['iddx_3'])\n",
    "            df['iddx_5'] = df['iddx_5'].fillna(df['iddx_4'])\n",
    "        elif self.mode != 'train':\n",
    "            dis = train_data.annotations['iddx_full'].value_counts(normalize=True)\n",
    "            generated_lesion_ids = np.random.choice(dis.index, size=len(df), p=dis.values)\n",
    "            df['iddx_full'] = generated_lesion_ids\n",
    "            for d in ['iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5']:\n",
    "                df[d] = df['iddx_full']\n",
    "        if self.mode != 'train':\n",
    "            tbp_lv_dnn_lesion_confidence_mean = train_data.annotations['tbp_lv_dnn_lesion_confidence'].mean()\n",
    "            tbp_lv_dnn_lesion_confidence_std = train_data.annotations['tbp_lv_dnn_lesion_confidence'].std()\n",
    "\n",
    "            df['tbp_lv_dnn_lesion_confidence'] = np.random.normal(loc=tbp_lv_dnn_lesion_confidence_mean, scale=tbp_lv_dnn_lesion_confidence_std, size = len(df))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def encode_labels(self, train_data=None):\n",
    "        if self.mode == 'train':\n",
    "            for col in self.categorical_vars:\n",
    "                le = LabelEncoder()\n",
    "                self.annotations[col] = le.fit_transform(self.annotations[col])\n",
    "                self.label_encoders[col] = le\n",
    "        elif self.mode != 'train' and train_data is not None:\n",
    "            for col in self.categorical_vars:\n",
    "                le = train_data.label_encoders[col]\n",
    "                mask = ~self.annotations[col].isin(le.classes_)\n",
    "    \n",
    "                # 对新标签进行随机替换\n",
    "                if mask.any():\n",
    "                    self.annotations.loc[mask, col] = np.random.choice(\n",
    "                        le.classes_, size=mask.sum(), p=train_data.annotations[col].value_counts(normalize=True).values\n",
    "                    )\n",
    "                \n",
    "                # 进行编码\n",
    "                self.annotations[col] = le.transform(self.annotations[col])\n",
    "                print(f\"Label encoder for {col} has {len(le.classes_)} classes\")\n",
    "\n",
    "                # self.annotations[col] = self.annotations[col].apply(\n",
    "                #     lambda x: x if x in le.classes_ else np.random.choice(le.classes_, p=train_data.annotations[col].value_counts(normalize=True).values)\n",
    "                # )\n",
    "                # self.annotations[col] = le.transform(self.annotations[col])\n",
    "                # print(f\"Label encoder for {col} has {len(le.classes_)} classes\")\n",
    "\n",
    "    def normalize_numerical_data(self, train_data=None):\n",
    "        if self.mode == 'train':\n",
    "            scaler = MinMaxScaler()\n",
    "            self.annotations[self.numerical_vars] = scaler.fit_transform(self.annotations[self.numerical_vars])\n",
    "        elif self.mode != 'train' and train_data is not None:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_data.annotations[train_data.numerical_vars])\n",
    "            self.annotations[self.numerical_vars] = scaler.transform(self.annotations[self.numerical_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4137629/2728867995.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(\"../../data/train-metadata.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data init begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4137629/2876828260.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_4137629/2876828260.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data init done\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../../data/train-metadata.csv\")\n",
    "mytransform = get_transform()\n",
    "train_set = CustomImageDataset(csv_file=train_df, hdf5_file=\"../../data/train-image.hdf5\", transform=mytransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_set.annotations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag = df[df['target'] == 1].copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['target', 'isic_id', 'patient_id', 'mel_thick_mm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = df.drop(columns=['target', 'isic_id', 'patient_id', 'mel_thick_mm']).to_numpy()\n",
    "y_np = df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vars = ['sex', 'anatom_site_general', 'image_type', 'tbp_tile_type', 'tbp_lv_location', \n",
    "                                 'tbp_lv_location_simple', 'attribution', 'copyright_license', 'lesion_id', \n",
    "                                 'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5', 'mel_mitotic_index']\n",
    "\n",
    "numerical_vars = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', \n",
    "                               'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n",
    "                               'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n",
    "                               'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', \n",
    "                               'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', \n",
    "                               'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', \n",
    "                               'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', \n",
    "                               'tbp_lv_z', 'tbp_lv_dnn_lesion_confidence']\n",
    "\n",
    "len(categorical_vars) + len(numerical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 25, 26, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_indices = [X.columns.get_loc(col) for col in categorical_vars if col in X.columns]\n",
    "# categorical_indices = [ 1 if col in categorical_vars  else 0  for  col in df.columns]\n",
    "\n",
    "categorical_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before resampling: (401059, 51)\n",
      "Shape of y before resampling: (401059,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X before resampling: {X.shape}\")\n",
    "print(f\"Shape of y before resampling: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({np.int64(0): 400666, np.int64(1): 80133})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# 假设你希望少数类样本占总样本的 20%\n",
    "sampling_strategy = 0.2\n",
    "\n",
    "smote_nc = SMOTENC(categorical_features=categorical_indices, sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_res, y_res = smote_nc.fit_resample(X_np, y_np)\n",
    "\n",
    "print(f'Resampled dataset shape: {Counter(y_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480799, 51)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y_res == 1\n",
    "\n",
    "# 2. 提取 X_res 中对应的样本\n",
    "X_res_minority = X_res[mask]\n",
    "\n",
    "# 3. 提取 y_res 中等于 1 的部分\n",
    "y_res_minority = y_res[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({np.int64(0): 400666, np.int64(1): 80133})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled data saved to resampled_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 假设 X_res 是 (n_samples, n_features) 的 ndarray\n",
    "# 假设 y_res 是 (n_samples,) 的 ndarray\n",
    "\n",
    "# 1. 将 X_res 转换为 DataFrame\n",
    "X_res_df = pd.DataFrame(X_res_minority[:10100], columns=X.columns)\n",
    "\n",
    "# 2. 将 y_res 添加为目标列\n",
    "X_res_df['target'] = y_res_minority[:10100]\n",
    "\n",
    "# 3. 保存 DataFrame 为 CSV 文件\n",
    "output_file = 'resampled_data.csv'\n",
    "X_res_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Resampled data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape (1000, 20)\n",
      "Original dataset samples per class Counter({np.int64(1): 900, np.int64(0): 100})\n",
      "Resampled dataset samples per class Counter({np.int64(0): 900, np.int64(1): 900})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from numpy.random import RandomState\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print(f'Original dataset shape {X.shape}')\n",
    "print(f'Original dataset samples per class {Counter(y)}')\n",
    "# simulate the 2 last columns to be categorical features\n",
    "X[:, -2:] = RandomState(10).randint(0, 4, size=(1000, 2))\n",
    "sm = SMOTENC(random_state=42, categorical_features=[18, 19])\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print(f'Resampled dataset samples per class {Counter(y_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after resampling: (1800, 20)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X after resampling: {X_res.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
