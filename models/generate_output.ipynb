{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import h5py\n",
    "import io\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, hdf5_file, transform=None, mode='train', train_data=None):\n",
    "        # self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.label_encoders = {}\n",
    "        self.mode = mode\n",
    "        self.categorical_vars = ['sex', 'anatom_site_general',\n",
    "                             'image_type', 'tbp_tile_type', 'tbp_lv_location', \n",
    "                             'tbp_lv_location_simple', 'attribution', 'copyright_license', \n",
    "                             'lesion_id', 'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', \n",
    "                             'iddx_4', 'iddx_5', 'mel_mitotic_index']\n",
    "        \n",
    "        self.numerical_vars = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', \n",
    "                          'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', \n",
    "                          'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n",
    "                          'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', \n",
    "                          'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', \n",
    "                          'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', \n",
    "                          'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', \n",
    "                          'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', \n",
    "                          'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', \n",
    "                          'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', \n",
    "                          'tbp_lv_y', 'tbp_lv_z', 'tbp_lv_dnn_lesion_confidence']\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.annotations = self.get_full_dataframe(csv_file)\n",
    "            self.encode_labels()\n",
    "            self.normalize_numerical_data()\n",
    "        elif mode == 'val' and train_data is not None:\n",
    "            self.annotations = self.get_full_dataframe(csv_file, train_data=train_data)\n",
    "            self.encode_labels(train_data)\n",
    "            self.normalize_numerical_data(train_data)\n",
    "\n",
    "        # print(numerical_df.isnull().sum())\n",
    "        # print(categorical_df.isnull().sum())\n",
    "        # print(numerical_df.values.dtype)\n",
    "        # print(categorical_df.values.dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        # img_path = os.path.join(self.img_dir, self.annotations.iloc[idx][\"isic_id\"] + \".jpg\")\n",
    "        # image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        with h5py.File(self.hdf5_file, 'r') as f:\n",
    "            isic_id = self.annotations.iloc[idx][\"isic_id\"]\n",
    "\n",
    "            if isic_id in f:\n",
    "                image = f[isic_id]\n",
    "                # Check if the data is numerical before conversion\n",
    "                image_data = image[()]\n",
    "                # 将字节字符串解码为图像\n",
    "                image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "                \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load label\n",
    "        label = torch.tensor(int(self.annotations.iloc[idx][\"target\"]))\n",
    "\n",
    "        # Load categorical data\n",
    "\n",
    "        numerical_df = self.annotations[self.numerical_vars]\n",
    "        categorical_df = self.annotations[self.categorical_vars]\n",
    "        numerical_df = numerical_df.iloc[idx]\n",
    "        categorical_df = categorical_df.iloc[idx]\n",
    "\n",
    "        numerical_data = torch.tensor(numerical_df.values, dtype=torch.float)\n",
    "        categorical_data = torch.tensor(categorical_df.values, dtype=torch.long)\n",
    "\n",
    "        # Load numerical data\n",
    "        \n",
    "\n",
    "        return image, categorical_data, numerical_data, label\n",
    "\n",
    "\n",
    "    def get_full_dataframe(self, path, train_data=None):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        def fill_missing_with_distribution(series, distribution):\n",
    "            missing_indices = series[series.isna()].index\n",
    "            filled_values = np.random.choice(distribution.index, size=len(missing_indices), p=distribution.values)\n",
    "            series.loc[missing_indices] = filled_values\n",
    "            return series\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            df['lesion_id'] = df['lesion_id'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "        elif self.mode == 'val':\n",
    "            for category in ['lesion_id', 'mel_mitotic_index']:\n",
    "                dis = train_data.annotations[category].value_counts(normalize=True)\n",
    "                generated_lesion_ids = np.random.choice(dis.index, size=len(df), p=dis.values)\n",
    "                df[category] = generated_lesion_ids\n",
    "\n",
    "        for category in ['sex', 'anatom_site_general']:\n",
    "            dis = df[category].value_counts(normalize=True)\n",
    "            df[category] = fill_missing_with_distribution(df[category], dis)\n",
    "        \n",
    "        mean_age = df['age_approx'].mean()\n",
    "        df['age_approx'] = df['age_approx'].fillna(mean_age)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            df['iddx_2'] = df['iddx_2'].fillna(df['iddx_1'])\n",
    "            df['iddx_3'] = df['iddx_3'].fillna(df['iddx_2'])\n",
    "            df['iddx_4'] = df['iddx_4'].fillna(df['iddx_3'])\n",
    "            df['iddx_5'] = df['iddx_5'].fillna(df['iddx_4'])\n",
    "        elif self.mode == 'val':\n",
    "            dis = train_data.annotations['iddx_full'].value_counts(normalize=True)\n",
    "            generated_lesion_ids = np.random.choice(dis.index, size=len(df), p=dis.values)\n",
    "            df['iddx_full'] = generated_lesion_ids\n",
    "            for d in ['iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5']:\n",
    "                df[d] = df['iddx_full']\n",
    "        if self.mode == 'val':\n",
    "            tbp_lv_dnn_lesion_confidence_mean = train_data.annotations['tbp_lv_dnn_lesion_confidence'].mean()\n",
    "            tbp_lv_dnn_lesion_confidence_std = train_data.annotations['tbp_lv_dnn_lesion_confidence'].std()\n",
    "\n",
    "            df['tbp_lv_dnn_lesion_confidence'] = np.random.normal(loc=tbp_lv_dnn_lesion_confidence_mean, scale=tbp_lv_dnn_lesion_confidence_std, size = len(df))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def encode_labels(self, train_data=None):\n",
    "        if self.mode == 'train':\n",
    "            for col in self.categorical_vars:\n",
    "                le = LabelEncoder()\n",
    "                self.annotations[col] = le.fit_transform(self.annotations[col])\n",
    "                self.label_encoders[col] = le\n",
    "        elif self.mode == 'val' and train_data is not None:\n",
    "            for col in self.categorical_vars:\n",
    "                le = train_data.label_encoders[col]\n",
    "                self.annotations[col] = self.annotations[col].apply(\n",
    "                    lambda x: x if x in le.classes_ else np.random.choice(le.classes_, p=train_data.annotations[col].value_counts(normalize=True).values)\n",
    "                )\n",
    "                self.annotations[col] = le.transform(self.annotations[col])\n",
    "\n",
    "    def normalize_numerical_data(self, train_data=None):\n",
    "        if self.mode == 'train':\n",
    "            scaler = MinMaxScaler()\n",
    "            self.annotations[self.numerical_vars] = scaler.fit_transform(self.annotations[self.numerical_vars])\n",
    "        elif self.mode == 'val' and train_data is not None:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_data.annotations[train_data.numerical_vars])\n",
    "            self.annotations[self.numerical_vars] = scaler.transform(self.annotations[self.numerical_vars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1699575/3151810075.py:105: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n",
      "/tmp/ipykernel_1699575/3151810075.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_1699575/3151810075.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n"
     ]
    }
   ],
   "source": [
    "mytransform = get_transform()\n",
    "train_set = CustomImageDataset(csv_file=\"../data/train-metadata.csv\", hdf5_file=\"../data/train-image.hdf5\", transform=mytransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1699575/3151810075.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_1699575/3151810075.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomImageDataset(csv_file='../data/test-metadata.csv', hdf5_file='../data/test-image.hdf5', transform=mytransform, mode='val', train_data=train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import h5py\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes, categorical_dims, num_numerical_features):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # 冻结 EfficientNet 的卷积层\n",
    "        for param in self.efficientnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 分类器部分\n",
    "        in_features = self.efficientnet._fc.in_features \n",
    "        self.efficientnet._fc = nn.Identity()  # 移除原来的全连接层\n",
    "        self.image_fc = nn.Linear(in_features, 512)\n",
    "        \n",
    "        # # 处理类别数据\n",
    "        # self.categorical_embeddings = nn.ModuleList([\n",
    "        #     nn.Embedding(num_embeddings=10, embedding_dim=5) for _ in range(num_categorical_features)\n",
    "        # ])\n",
    "\n",
    "        self.categorical_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=dim+1, embedding_dim=min(50, (dim + 1) // 2))\n",
    "            for dim in categorical_dims\n",
    "        ])\n",
    "\n",
    "        categorical_total_dim = sum([embedding.embedding_dim for embedding in self.categorical_embeddings])\n",
    "\n",
    "        self.categorical_fc = nn.Linear(categorical_total_dim, 32)\n",
    "        \n",
    "        # 处理数值数据\n",
    "        self.numerical_fc = nn.Linear(num_numerical_features, 32)\n",
    "        \n",
    "        # 最终分类器\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(512 + 32 + 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, categorical_data, numerical_data):\n",
    "        # 图像特征\n",
    "        x_image = self.efficientnet(images)\n",
    "        x_image = self.image_fc(x_image)\n",
    "        \n",
    "        # 类别特征\n",
    "        # print(f\"Shape of categorical_data: {categorical_data.shape}\")\n",
    "        x_categorical = [embedding(categorical_data[:, i]) for i, embedding in enumerate(self.categorical_embeddings)]\n",
    "        # for i, tensor in enumerate(x_categorical):\n",
    "        #     print(f\"Shape of tensor {i}: {tensor.shape}\")   \n",
    "\n",
    "        x_categorical = torch.cat(x_categorical, dim=1)\n",
    "        x_categorical = self.categorical_fc(x_categorical)\n",
    "        \n",
    "        # 数值特征\n",
    "        x_numerical = self.numerical_fc(numerical_data)\n",
    "        \n",
    "        # 结合所有特征\n",
    "        x = torch.cat((x_image, x_categorical, x_numerical), dim=1)\n",
    "        x = self.final_fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def get_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, hdf5_file, transform=None, mode='train', train_data=None):\n",
    "        # self.img_dir = img_dir\n",
    "        self.mode = mode\n",
    "        if self.mode == \"train\":\n",
    "            print(\"train data init begin\")\n",
    "        elif self.mode == \"val\":\n",
    "            print(\"val data init begin\")\n",
    "\n",
    "        self.transform = transform\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "        self.categorical_vars = ['sex', 'anatom_site_general',\n",
    "                             'image_type', 'tbp_tile_type', 'tbp_lv_location', \n",
    "                             'tbp_lv_location_simple', 'attribution', 'copyright_license', \n",
    "                             'lesion_id', 'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', \n",
    "                             'iddx_4', 'iddx_5', 'mel_mitotic_index']\n",
    "        \n",
    "        self.numerical_vars = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', \n",
    "                          'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', \n",
    "                          'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n",
    "                          'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', \n",
    "                          'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', \n",
    "                          'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', \n",
    "                          'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', \n",
    "                          'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', \n",
    "                          'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', \n",
    "                          'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', \n",
    "                          'tbp_lv_y', 'tbp_lv_z', 'tbp_lv_dnn_lesion_confidence']\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.annotations = self.get_full_dataframe(csv_file)\n",
    "            self.encode_labels()\n",
    "            self.normalize_numerical_data()\n",
    "        elif mode == 'val' or mode == 'test' and train_data is not None:\n",
    "            print(\"getting full dataframe\")\n",
    "            self.annotations = self.get_full_dataframe(csv_file, train_data=train_data)\n",
    "            print(\"encoding labels\")\n",
    "            self.encode_labels(train_data)\n",
    "            print(\"normalizing numerical data\")\n",
    "            self.normalize_numerical_data(train_data)\n",
    "\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            print(\"train data init done\")\n",
    "        elif self.mode == \"val\":\n",
    "            print(\"val data init done\")\n",
    "        # print(numerical_df.isnull().sum())\n",
    "        # print(categorical_df.isnull().sum())\n",
    "        # print(numerical_df.values.dtype)\n",
    "        # print(categorical_df.values.dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        # img_path = os.path.join(self.img_dir, self.annotations.iloc[idx][\"isic_id\"] + \".jpg\")\n",
    "        # image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        with h5py.File(self.hdf5_file, 'r') as f:\n",
    "            isic_id = self.annotations.iloc[idx][\"isic_id\"]\n",
    "\n",
    "            if isic_id in f:\n",
    "                image = f[isic_id]\n",
    "                # Check if the data is numerical before conversion\n",
    "                image_data = image[()]\n",
    "                # 将字节字符串解码为图像\n",
    "                image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "                \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load categorical data\n",
    "\n",
    "        numerical_df = self.annotations[self.numerical_vars]\n",
    "        categorical_df = self.annotations[self.categorical_vars]\n",
    "        numerical_df = numerical_df.iloc[idx]\n",
    "        categorical_df = categorical_df.iloc[idx]\n",
    "\n",
    "        numerical_data = torch.tensor(numerical_df.values, dtype=torch.float)\n",
    "        categorical_data = torch.tensor(categorical_df.values, dtype=torch.long)\n",
    "\n",
    "        # Load numerical data\n",
    "        if self.mode in ['train', 'val']:\n",
    "            label = torch.tensor(int(self.annotations.iloc[idx][\"target\"]))\n",
    "            return image, categorical_data, numerical_data, label\n",
    "\n",
    "        elif self.mode == 'test':\n",
    "            isic_id = self.annotations.iloc[idx][\"isic_id\"]\n",
    "            return image, categorical_data, numerical_data, isic_id\n",
    "        \n",
    "\n",
    "    def get_full_dataframe(self, df, train_data=None):\n",
    "        def fill_missing_with_distribution(series, distribution):\n",
    "            missing_indices = series[series.isna()].index\n",
    "            filled_values = np.random.choice(distribution.index, size=len(missing_indices), p=distribution.values)\n",
    "            series.loc[missing_indices] = filled_values\n",
    "            return series\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            df['lesion_id'] = df['lesion_id'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "        elif self.mode != 'train':\n",
    "            for category in ['lesion_id', 'mel_mitotic_index']:\n",
    "                dis = train_data.annotations[category].value_counts(normalize=True)\n",
    "                generated_lesion_ids = np.random.choice(dis.index, size=len(df), p=dis.values)\n",
    "                df[category] = generated_lesion_ids\n",
    "\n",
    "        for category in ['sex', 'anatom_site_general']:\n",
    "            dis = df[category].value_counts(normalize=True)\n",
    "            df[category] = fill_missing_with_distribution(df[category], dis)\n",
    "        \n",
    "        mean_age = df['age_approx'].mean()\n",
    "        df['age_approx'] = df['age_approx'].fillna(mean_age)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            df['iddx_2'] = df['iddx_2'].fillna(df['iddx_1'])\n",
    "            df['iddx_3'] = df['iddx_3'].fillna(df['iddx_2'])\n",
    "            df['iddx_4'] = df['iddx_4'].fillna(df['iddx_3'])\n",
    "            df['iddx_5'] = df['iddx_5'].fillna(df['iddx_4'])\n",
    "        elif self.mode != 'train':\n",
    "            dis = train_data.annotations['iddx_full'].value_counts(normalize=True)\n",
    "            generated_lesion_ids = np.random.choice(dis.index, size=len(df), p=dis.values)\n",
    "            df['iddx_full'] = generated_lesion_ids\n",
    "            for d in ['iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5']:\n",
    "                df[d] = df['iddx_full']\n",
    "        if self.mode != 'train':\n",
    "            tbp_lv_dnn_lesion_confidence_mean = train_data.annotations['tbp_lv_dnn_lesion_confidence'].mean()\n",
    "            tbp_lv_dnn_lesion_confidence_std = train_data.annotations['tbp_lv_dnn_lesion_confidence'].std()\n",
    "\n",
    "            df['tbp_lv_dnn_lesion_confidence'] = np.random.normal(loc=tbp_lv_dnn_lesion_confidence_mean, scale=tbp_lv_dnn_lesion_confidence_std, size = len(df))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def encode_labels(self, train_data=None):\n",
    "        if self.mode == 'train':\n",
    "            for col in self.categorical_vars:\n",
    "                le = LabelEncoder()\n",
    "                self.annotations[col] = le.fit_transform(self.annotations[col])\n",
    "                self.label_encoders[col] = le\n",
    "        elif self.mode != 'train' and train_data is not None:\n",
    "            for col in self.categorical_vars:\n",
    "                le = train_data.label_encoders[col]\n",
    "                mask = ~self.annotations[col].isin(le.classes_)\n",
    "    \n",
    "                # 对新标签进行随机替换\n",
    "                if mask.any():\n",
    "                    self.annotations.loc[mask, col] = np.random.choice(\n",
    "                        le.classes_, size=mask.sum(), p=train_data.annotations[col].value_counts(normalize=True).values\n",
    "                    )\n",
    "                \n",
    "                # 进行编码\n",
    "                self.annotations[col] = le.transform(self.annotations[col])\n",
    "                print(f\"Label encoder for {col} has {len(le.classes_)} classes\")\n",
    "\n",
    "                # self.annotations[col] = self.annotations[col].apply(\n",
    "                #     lambda x: x if x in le.classes_ else np.random.choice(le.classes_, p=train_data.annotations[col].value_counts(normalize=True).values)\n",
    "                # )\n",
    "                # self.annotations[col] = le.transform(self.annotations[col])\n",
    "                # print(f\"Label encoder for {col} has {len(le.classes_)} classes\")\n",
    "\n",
    "    def normalize_numerical_data(self, train_data=None):\n",
    "        if self.mode == 'train':\n",
    "            scaler = MinMaxScaler()\n",
    "            self.annotations[self.numerical_vars] = scaler.fit_transform(self.annotations[self.numerical_vars])\n",
    "        elif self.mode != 'train' and train_data is not None:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_data.annotations[train_data.numerical_vars])\n",
    "            self.annotations[self.numerical_vars] = scaler.transform(self.annotations[self.numerical_vars])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_output(model_path):\n",
    "    mytransform = get_transform()\n",
    "    csv_file = pd.read_csv(\"../data/train-metadata.csv\")\n",
    "    train_df, val_df = train_test_split(csv_file, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_set = CustomImageDataset(csv_file=train_df, hdf5_file=\"../data/train-image.hdf5\", transform=mytransform)\n",
    "    val_set = CustomImageDataset(csv_file=val_df, hdf5_file=\"../data/train-image.hdf5\", transform=mytransform, mode='val', train_data=train_set)\n",
    "    test_set = CustomImageDataset(csv_file=pd.read_csv(\"../data/test-metadata.csv\"), hdf5_file=\"../data/test-image.hdf5\", transform=mytransform, mode='test', train_data=train_set)\n",
    "\n",
    "\n",
    "    categorical_dims = [2, 5, 1, 2, 21, 8, 7, 3, 2, 52, 3, 15, 28, 52, 52, 7]\n",
    "    model = CombinedModel(2, categorical_dims, 35)  # load your model here\n",
    "\n",
    "    # 加载 state_dict\n",
    "    state_dict = torch.load(\"best_model.pt\")\n",
    "\n",
    "    # 创建一个新的 state_dict，将键名中的 'module.' 前缀移除\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace(\"module.\", \"\")  # 移除 'module.' 前缀\n",
    "        new_state_dict[new_key] = v\n",
    "\n",
    "    # 加载新的 state_dict 到模型\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    return model, val_set\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/2038448372.py:3: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_file = pd.read_csv(\"../data/train-metadata.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data init begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_1888154/3751047009.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data init done\n",
      "val data init begin\n",
      "getting full dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_1888154/3751047009.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding labels\n",
      "Label encoder for sex has 2 classes\n",
      "Label encoder for anatom_site_general has 5 classes\n",
      "Label encoder for image_type has 1 classes\n",
      "Label encoder for tbp_tile_type has 2 classes\n",
      "Label encoder for tbp_lv_location has 21 classes\n",
      "Label encoder for tbp_lv_location_simple has 8 classes\n",
      "Label encoder for attribution has 7 classes\n",
      "Label encoder for copyright_license has 3 classes\n",
      "Label encoder for lesion_id has 2 classes\n",
      "Label encoder for iddx_full has 50 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Benign' 'Benign' 'Benign' ... 'Benign' 'Benign' 'Benign']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Benign' 'Benign' 'Benign' ... 'Benign' 'Benign' 'Benign']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder for iddx_1 has 3 classes\n",
      "Label encoder for iddx_2 has 14 classes\n",
      "Label encoder for iddx_3 has 27 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Benign' 'Benign' 'Benign' ... 'Benign' 'Benign' 'Benign']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Angiofibroma' 'Angiofibroma' 'Angiofibroma' ... 'Angiofibroma'\n",
      " 'Angiofibroma' 'Angiofibroma']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Angiofibroma, Facial' 'Angiofibroma, Facial' 'Angiofibroma, Facial' ...\n",
      " 'Angiofibroma, Facial' 'Angiofibroma, Facial' 'Angiofibroma, Facial']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder for iddx_4 has 50 classes\n",
      "Label encoder for iddx_5 has 50 classes\n",
      "Label encoder for mel_mitotic_index has 8 classes\n",
      "normalizing numerical data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Angiofibroma, Facial' 'Angiofibroma, Facial' 'Angiofibroma, Facial' ...\n",
      " 'Angiofibroma, Facial' 'Angiofibroma, Facial' 'Angiofibroma, Facial']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['0/mm^2' '0/mm^2' '0/mm^2' ... '0/mm^2' '0/mm^2' '0/mm^2']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val data init done\n",
      "getting full dataframe\n",
      "encoding labels\n",
      "Label encoder for sex has 2 classes\n",
      "Label encoder for anatom_site_general has 5 classes\n",
      "Label encoder for image_type has 1 classes\n",
      "Label encoder for tbp_tile_type has 2 classes\n",
      "Label encoder for tbp_lv_location has 21 classes\n",
      "Label encoder for tbp_lv_location_simple has 8 classes\n",
      "Label encoder for attribution has 7 classes\n",
      "Label encoder for copyright_license has 3 classes\n",
      "Label encoder for lesion_id has 2 classes\n",
      "Label encoder for iddx_full has 50 classes\n",
      "Label encoder for iddx_1 has 3 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_1888154/3751047009.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series.loc[missing_indices] = filled_values\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Benign' 'Benign' 'Benign']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Benign' 'Benign' 'Benign']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Benign' 'Benign' 'Benign']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Angiofibroma' 'Angiofibroma' 'Angiofibroma']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Angiofibroma, Facial' 'Angiofibroma, Facial' 'Angiofibroma, Facial']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder for iddx_2 has 14 classes\n",
      "Label encoder for iddx_3 has 27 classes\n",
      "Label encoder for iddx_4 has 50 classes\n",
      "Label encoder for iddx_5 has 50 classes\n",
      "Label encoder for mel_mitotic_index has 8 classes\n",
      "normalizing numerical data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Angiofibroma, Facial' 'Angiofibroma, Facial' 'Angiofibroma, Facial']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n",
      "/tmp/ipykernel_1888154/3751047009.py:249: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['0/mm^2' '0/mm^2' '0/mm^2']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.annotations.loc[mask, col] = np.random.choice(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2507/2507 [20:47<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2541, Accuracy: 0.9741, pAUC: 1.5876\n"
     ]
    }
   ],
   "source": [
    "model, val_set = generate_output(\"best_model.pt\")\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "# test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "results = []\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "with torch.no_grad():\n",
    "    results = []\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for images, categorical_data, numerical_data, labels in tqdm(val_loader):\n",
    "        images = images.cuda()\n",
    "        categorical_data = categorical_data.cuda()\n",
    "        numerical_data = numerical_data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(images, categorical_data, numerical_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        all_probs.extend(probabilities[:, 1].cpu().numpy())  # Probabilities of the positive class\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "\n",
    "    # Calculate pAUC\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    tpr_threshold = 0.8\n",
    "    mask = tpr >= tpr_threshold\n",
    "    fpr_filtered = fpr[mask]\n",
    "    tpr_filtered = tpr[mask]\n",
    "    pAUC = auc(fpr_filtered, tpr_filtered)\n",
    "    pAUC_normalized = pAUC / (tpr_filtered[-1] - tpr_threshold)\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, pAUC: {pAUC_normalized:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2541, Accuracy: 0.9741, pAUC: -0.0284\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "tpr_threshold = 0.8\n",
    "mask = (fpr >= 0) & (fpr <= 0.1)\n",
    "fpr_filtered = fpr[mask]\n",
    "tpr_filtered = tpr[mask]\n",
    "pAUC = auc(fpr_filtered, tpr_filtered)\n",
    "pAUC_normalized = pAUC / (tpr_filtered[-1] - tpr_threshold)\n",
    "print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, pAUC: {pAUC_normalized:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized pAUC above 80% TPR is: 0.31752642232689154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1888154/385834960.py:20: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  pauc = np.trapz(tpr_above, fpr_above)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def compute_pauc_above_tpr(y_true, y_scores, tpr_threshold=0.8):\n",
    "    # Step 1: Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "    # Step 2: Filter out TPR < 0.8\n",
    "    indices_above_tpr = np.where(tpr >= tpr_threshold)[0]\n",
    "    \n",
    "    # If no TPR values are above the threshold, return 0\n",
    "    if len(indices_above_tpr) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Select the portion of the curve where TPR >= 0.8\n",
    "    fpr_above = fpr[indices_above_tpr]\n",
    "    tpr_above = tpr[indices_above_tpr]\n",
    "    \n",
    "    # Step 3: Calculate pAUC using the trapezoidal rule\n",
    "    pauc = np.trapz(tpr_above, fpr_above)\n",
    "    \n",
    "    # Normalize the pAUC by dividing by the maximum possible pAUC in this range\n",
    "    max_pauc = 0.2  # Because TPR range is from 0.8 to 1, and max FPR range would be 0 to 1\n",
    "    pauc_normalized = pauc / max_pauc\n",
    "    return pauc\n",
    "    return pauc_normalized\n",
    "\n",
    "# Example usage:\n",
    "# y_true: array of true binary labels (0 or 1)\n",
    "# y_scores: array of predicted probabilities for the positive class\n",
    "pauc_score = compute_pauc_above_tpr(all_labels, all_probs)\n",
    "print(f\"The normalized pAUC above 80% TPR is: {pauc_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "假阴性的数量 (FN): 8\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = (all_probs >= 0.5).astype(int)\n",
    "false_negatives = np.sum((all_labels == 1) & (predicted_labels == 1))\n",
    "\n",
    "# 输出假阴性的数量\n",
    "print(f\"假阴性的数量 (FN): {false_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(all_labels == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight\n",
      "features.0.1.weight\n",
      "features.0.1.bias\n",
      "features.0.1.running_mean\n",
      "features.0.1.running_var\n",
      "features.0.1.num_batches_tracked\n",
      "features.1.0.block.0.0.weight\n",
      "features.1.0.block.0.1.weight\n",
      "features.1.0.block.0.1.bias\n",
      "features.1.0.block.0.1.running_mean\n",
      "features.1.0.block.0.1.running_var\n",
      "features.1.0.block.0.1.num_batches_tracked\n",
      "features.1.0.block.1.fc1.weight\n",
      "features.1.0.block.1.fc1.bias\n",
      "features.1.0.block.1.fc2.weight\n",
      "features.1.0.block.1.fc2.bias\n",
      "features.1.0.block.2.0.weight\n",
      "features.1.0.block.2.1.weight\n",
      "features.1.0.block.2.1.bias\n",
      "features.1.0.block.2.1.running_mean\n",
      "features.1.0.block.2.1.running_var\n",
      "features.1.0.block.2.1.num_batches_tracked\n",
      "features.2.0.block.0.0.weight\n",
      "features.2.0.block.0.1.weight\n",
      "features.2.0.block.0.1.bias\n",
      "features.2.0.block.0.1.running_mean\n",
      "features.2.0.block.0.1.running_var\n",
      "features.2.0.block.0.1.num_batches_tracked\n",
      "features.2.0.block.1.0.weight\n",
      "features.2.0.block.1.1.weight\n",
      "features.2.0.block.1.1.bias\n",
      "features.2.0.block.1.1.running_mean\n",
      "features.2.0.block.1.1.running_var\n",
      "features.2.0.block.1.1.num_batches_tracked\n",
      "features.2.0.block.2.fc1.weight\n",
      "features.2.0.block.2.fc1.bias\n",
      "features.2.0.block.2.fc2.weight\n",
      "features.2.0.block.2.fc2.bias\n",
      "features.2.0.block.3.0.weight\n",
      "features.2.0.block.3.1.weight\n",
      "features.2.0.block.3.1.bias\n",
      "features.2.0.block.3.1.running_mean\n",
      "features.2.0.block.3.1.running_var\n",
      "features.2.0.block.3.1.num_batches_tracked\n",
      "features.2.1.block.0.0.weight\n",
      "features.2.1.block.0.1.weight\n",
      "features.2.1.block.0.1.bias\n",
      "features.2.1.block.0.1.running_mean\n",
      "features.2.1.block.0.1.running_var\n",
      "features.2.1.block.0.1.num_batches_tracked\n",
      "features.2.1.block.1.0.weight\n",
      "features.2.1.block.1.1.weight\n",
      "features.2.1.block.1.1.bias\n",
      "features.2.1.block.1.1.running_mean\n",
      "features.2.1.block.1.1.running_var\n",
      "features.2.1.block.1.1.num_batches_tracked\n",
      "features.2.1.block.2.fc1.weight\n",
      "features.2.1.block.2.fc1.bias\n",
      "features.2.1.block.2.fc2.weight\n",
      "features.2.1.block.2.fc2.bias\n",
      "features.2.1.block.3.0.weight\n",
      "features.2.1.block.3.1.weight\n",
      "features.2.1.block.3.1.bias\n",
      "features.2.1.block.3.1.running_mean\n",
      "features.2.1.block.3.1.running_var\n",
      "features.2.1.block.3.1.num_batches_tracked\n",
      "features.3.0.block.0.0.weight\n",
      "features.3.0.block.0.1.weight\n",
      "features.3.0.block.0.1.bias\n",
      "features.3.0.block.0.1.running_mean\n",
      "features.3.0.block.0.1.running_var\n",
      "features.3.0.block.0.1.num_batches_tracked\n",
      "features.3.0.block.1.0.weight\n",
      "features.3.0.block.1.1.weight\n",
      "features.3.0.block.1.1.bias\n",
      "features.3.0.block.1.1.running_mean\n",
      "features.3.0.block.1.1.running_var\n",
      "features.3.0.block.1.1.num_batches_tracked\n",
      "features.3.0.block.2.fc1.weight\n",
      "features.3.0.block.2.fc1.bias\n",
      "features.3.0.block.2.fc2.weight\n",
      "features.3.0.block.2.fc2.bias\n",
      "features.3.0.block.3.0.weight\n",
      "features.3.0.block.3.1.weight\n",
      "features.3.0.block.3.1.bias\n",
      "features.3.0.block.3.1.running_mean\n",
      "features.3.0.block.3.1.running_var\n",
      "features.3.0.block.3.1.num_batches_tracked\n",
      "features.3.1.block.0.0.weight\n",
      "features.3.1.block.0.1.weight\n",
      "features.3.1.block.0.1.bias\n",
      "features.3.1.block.0.1.running_mean\n",
      "features.3.1.block.0.1.running_var\n",
      "features.3.1.block.0.1.num_batches_tracked\n",
      "features.3.1.block.1.0.weight\n",
      "features.3.1.block.1.1.weight\n",
      "features.3.1.block.1.1.bias\n",
      "features.3.1.block.1.1.running_mean\n",
      "features.3.1.block.1.1.running_var\n",
      "features.3.1.block.1.1.num_batches_tracked\n",
      "features.3.1.block.2.fc1.weight\n",
      "features.3.1.block.2.fc1.bias\n",
      "features.3.1.block.2.fc2.weight\n",
      "features.3.1.block.2.fc2.bias\n",
      "features.3.1.block.3.0.weight\n",
      "features.3.1.block.3.1.weight\n",
      "features.3.1.block.3.1.bias\n",
      "features.3.1.block.3.1.running_mean\n",
      "features.3.1.block.3.1.running_var\n",
      "features.3.1.block.3.1.num_batches_tracked\n",
      "features.4.0.block.0.0.weight\n",
      "features.4.0.block.0.1.weight\n",
      "features.4.0.block.0.1.bias\n",
      "features.4.0.block.0.1.running_mean\n",
      "features.4.0.block.0.1.running_var\n",
      "features.4.0.block.0.1.num_batches_tracked\n",
      "features.4.0.block.1.0.weight\n",
      "features.4.0.block.1.1.weight\n",
      "features.4.0.block.1.1.bias\n",
      "features.4.0.block.1.1.running_mean\n",
      "features.4.0.block.1.1.running_var\n",
      "features.4.0.block.1.1.num_batches_tracked\n",
      "features.4.0.block.2.fc1.weight\n",
      "features.4.0.block.2.fc1.bias\n",
      "features.4.0.block.2.fc2.weight\n",
      "features.4.0.block.2.fc2.bias\n",
      "features.4.0.block.3.0.weight\n",
      "features.4.0.block.3.1.weight\n",
      "features.4.0.block.3.1.bias\n",
      "features.4.0.block.3.1.running_mean\n",
      "features.4.0.block.3.1.running_var\n",
      "features.4.0.block.3.1.num_batches_tracked\n",
      "features.4.1.block.0.0.weight\n",
      "features.4.1.block.0.1.weight\n",
      "features.4.1.block.0.1.bias\n",
      "features.4.1.block.0.1.running_mean\n",
      "features.4.1.block.0.1.running_var\n",
      "features.4.1.block.0.1.num_batches_tracked\n",
      "features.4.1.block.1.0.weight\n",
      "features.4.1.block.1.1.weight\n",
      "features.4.1.block.1.1.bias\n",
      "features.4.1.block.1.1.running_mean\n",
      "features.4.1.block.1.1.running_var\n",
      "features.4.1.block.1.1.num_batches_tracked\n",
      "features.4.1.block.2.fc1.weight\n",
      "features.4.1.block.2.fc1.bias\n",
      "features.4.1.block.2.fc2.weight\n",
      "features.4.1.block.2.fc2.bias\n",
      "features.4.1.block.3.0.weight\n",
      "features.4.1.block.3.1.weight\n",
      "features.4.1.block.3.1.bias\n",
      "features.4.1.block.3.1.running_mean\n",
      "features.4.1.block.3.1.running_var\n",
      "features.4.1.block.3.1.num_batches_tracked\n",
      "features.4.2.block.0.0.weight\n",
      "features.4.2.block.0.1.weight\n",
      "features.4.2.block.0.1.bias\n",
      "features.4.2.block.0.1.running_mean\n",
      "features.4.2.block.0.1.running_var\n",
      "features.4.2.block.0.1.num_batches_tracked\n",
      "features.4.2.block.1.0.weight\n",
      "features.4.2.block.1.1.weight\n",
      "features.4.2.block.1.1.bias\n",
      "features.4.2.block.1.1.running_mean\n",
      "features.4.2.block.1.1.running_var\n",
      "features.4.2.block.1.1.num_batches_tracked\n",
      "features.4.2.block.2.fc1.weight\n",
      "features.4.2.block.2.fc1.bias\n",
      "features.4.2.block.2.fc2.weight\n",
      "features.4.2.block.2.fc2.bias\n",
      "features.4.2.block.3.0.weight\n",
      "features.4.2.block.3.1.weight\n",
      "features.4.2.block.3.1.bias\n",
      "features.4.2.block.3.1.running_mean\n",
      "features.4.2.block.3.1.running_var\n",
      "features.4.2.block.3.1.num_batches_tracked\n",
      "features.5.0.block.0.0.weight\n",
      "features.5.0.block.0.1.weight\n",
      "features.5.0.block.0.1.bias\n",
      "features.5.0.block.0.1.running_mean\n",
      "features.5.0.block.0.1.running_var\n",
      "features.5.0.block.0.1.num_batches_tracked\n",
      "features.5.0.block.1.0.weight\n",
      "features.5.0.block.1.1.weight\n",
      "features.5.0.block.1.1.bias\n",
      "features.5.0.block.1.1.running_mean\n",
      "features.5.0.block.1.1.running_var\n",
      "features.5.0.block.1.1.num_batches_tracked\n",
      "features.5.0.block.2.fc1.weight\n",
      "features.5.0.block.2.fc1.bias\n",
      "features.5.0.block.2.fc2.weight\n",
      "features.5.0.block.2.fc2.bias\n",
      "features.5.0.block.3.0.weight\n",
      "features.5.0.block.3.1.weight\n",
      "features.5.0.block.3.1.bias\n",
      "features.5.0.block.3.1.running_mean\n",
      "features.5.0.block.3.1.running_var\n",
      "features.5.0.block.3.1.num_batches_tracked\n",
      "features.5.1.block.0.0.weight\n",
      "features.5.1.block.0.1.weight\n",
      "features.5.1.block.0.1.bias\n",
      "features.5.1.block.0.1.running_mean\n",
      "features.5.1.block.0.1.running_var\n",
      "features.5.1.block.0.1.num_batches_tracked\n",
      "features.5.1.block.1.0.weight\n",
      "features.5.1.block.1.1.weight\n",
      "features.5.1.block.1.1.bias\n",
      "features.5.1.block.1.1.running_mean\n",
      "features.5.1.block.1.1.running_var\n",
      "features.5.1.block.1.1.num_batches_tracked\n",
      "features.5.1.block.2.fc1.weight\n",
      "features.5.1.block.2.fc1.bias\n",
      "features.5.1.block.2.fc2.weight\n",
      "features.5.1.block.2.fc2.bias\n",
      "features.5.1.block.3.0.weight\n",
      "features.5.1.block.3.1.weight\n",
      "features.5.1.block.3.1.bias\n",
      "features.5.1.block.3.1.running_mean\n",
      "features.5.1.block.3.1.running_var\n",
      "features.5.1.block.3.1.num_batches_tracked\n",
      "features.5.2.block.0.0.weight\n",
      "features.5.2.block.0.1.weight\n",
      "features.5.2.block.0.1.bias\n",
      "features.5.2.block.0.1.running_mean\n",
      "features.5.2.block.0.1.running_var\n",
      "features.5.2.block.0.1.num_batches_tracked\n",
      "features.5.2.block.1.0.weight\n",
      "features.5.2.block.1.1.weight\n",
      "features.5.2.block.1.1.bias\n",
      "features.5.2.block.1.1.running_mean\n",
      "features.5.2.block.1.1.running_var\n",
      "features.5.2.block.1.1.num_batches_tracked\n",
      "features.5.2.block.2.fc1.weight\n",
      "features.5.2.block.2.fc1.bias\n",
      "features.5.2.block.2.fc2.weight\n",
      "features.5.2.block.2.fc2.bias\n",
      "features.5.2.block.3.0.weight\n",
      "features.5.2.block.3.1.weight\n",
      "features.5.2.block.3.1.bias\n",
      "features.5.2.block.3.1.running_mean\n",
      "features.5.2.block.3.1.running_var\n",
      "features.5.2.block.3.1.num_batches_tracked\n",
      "features.6.0.block.0.0.weight\n",
      "features.6.0.block.0.1.weight\n",
      "features.6.0.block.0.1.bias\n",
      "features.6.0.block.0.1.running_mean\n",
      "features.6.0.block.0.1.running_var\n",
      "features.6.0.block.0.1.num_batches_tracked\n",
      "features.6.0.block.1.0.weight\n",
      "features.6.0.block.1.1.weight\n",
      "features.6.0.block.1.1.bias\n",
      "features.6.0.block.1.1.running_mean\n",
      "features.6.0.block.1.1.running_var\n",
      "features.6.0.block.1.1.num_batches_tracked\n",
      "features.6.0.block.2.fc1.weight\n",
      "features.6.0.block.2.fc1.bias\n",
      "features.6.0.block.2.fc2.weight\n",
      "features.6.0.block.2.fc2.bias\n",
      "features.6.0.block.3.0.weight\n",
      "features.6.0.block.3.1.weight\n",
      "features.6.0.block.3.1.bias\n",
      "features.6.0.block.3.1.running_mean\n",
      "features.6.0.block.3.1.running_var\n",
      "features.6.0.block.3.1.num_batches_tracked\n",
      "features.6.1.block.0.0.weight\n",
      "features.6.1.block.0.1.weight\n",
      "features.6.1.block.0.1.bias\n",
      "features.6.1.block.0.1.running_mean\n",
      "features.6.1.block.0.1.running_var\n",
      "features.6.1.block.0.1.num_batches_tracked\n",
      "features.6.1.block.1.0.weight\n",
      "features.6.1.block.1.1.weight\n",
      "features.6.1.block.1.1.bias\n",
      "features.6.1.block.1.1.running_mean\n",
      "features.6.1.block.1.1.running_var\n",
      "features.6.1.block.1.1.num_batches_tracked\n",
      "features.6.1.block.2.fc1.weight\n",
      "features.6.1.block.2.fc1.bias\n",
      "features.6.1.block.2.fc2.weight\n",
      "features.6.1.block.2.fc2.bias\n",
      "features.6.1.block.3.0.weight\n",
      "features.6.1.block.3.1.weight\n",
      "features.6.1.block.3.1.bias\n",
      "features.6.1.block.3.1.running_mean\n",
      "features.6.1.block.3.1.running_var\n",
      "features.6.1.block.3.1.num_batches_tracked\n",
      "features.6.2.block.0.0.weight\n",
      "features.6.2.block.0.1.weight\n",
      "features.6.2.block.0.1.bias\n",
      "features.6.2.block.0.1.running_mean\n",
      "features.6.2.block.0.1.running_var\n",
      "features.6.2.block.0.1.num_batches_tracked\n",
      "features.6.2.block.1.0.weight\n",
      "features.6.2.block.1.1.weight\n",
      "features.6.2.block.1.1.bias\n",
      "features.6.2.block.1.1.running_mean\n",
      "features.6.2.block.1.1.running_var\n",
      "features.6.2.block.1.1.num_batches_tracked\n",
      "features.6.2.block.2.fc1.weight\n",
      "features.6.2.block.2.fc1.bias\n",
      "features.6.2.block.2.fc2.weight\n",
      "features.6.2.block.2.fc2.bias\n",
      "features.6.2.block.3.0.weight\n",
      "features.6.2.block.3.1.weight\n",
      "features.6.2.block.3.1.bias\n",
      "features.6.2.block.3.1.running_mean\n",
      "features.6.2.block.3.1.running_var\n",
      "features.6.2.block.3.1.num_batches_tracked\n",
      "features.6.3.block.0.0.weight\n",
      "features.6.3.block.0.1.weight\n",
      "features.6.3.block.0.1.bias\n",
      "features.6.3.block.0.1.running_mean\n",
      "features.6.3.block.0.1.running_var\n",
      "features.6.3.block.0.1.num_batches_tracked\n",
      "features.6.3.block.1.0.weight\n",
      "features.6.3.block.1.1.weight\n",
      "features.6.3.block.1.1.bias\n",
      "features.6.3.block.1.1.running_mean\n",
      "features.6.3.block.1.1.running_var\n",
      "features.6.3.block.1.1.num_batches_tracked\n",
      "features.6.3.block.2.fc1.weight\n",
      "features.6.3.block.2.fc1.bias\n",
      "features.6.3.block.2.fc2.weight\n",
      "features.6.3.block.2.fc2.bias\n",
      "features.6.3.block.3.0.weight\n",
      "features.6.3.block.3.1.weight\n",
      "features.6.3.block.3.1.bias\n",
      "features.6.3.block.3.1.running_mean\n",
      "features.6.3.block.3.1.running_var\n",
      "features.6.3.block.3.1.num_batches_tracked\n",
      "features.7.0.block.0.0.weight\n",
      "features.7.0.block.0.1.weight\n",
      "features.7.0.block.0.1.bias\n",
      "features.7.0.block.0.1.running_mean\n",
      "features.7.0.block.0.1.running_var\n",
      "features.7.0.block.0.1.num_batches_tracked\n",
      "features.7.0.block.1.0.weight\n",
      "features.7.0.block.1.1.weight\n",
      "features.7.0.block.1.1.bias\n",
      "features.7.0.block.1.1.running_mean\n",
      "features.7.0.block.1.1.running_var\n",
      "features.7.0.block.1.1.num_batches_tracked\n",
      "features.7.0.block.2.fc1.weight\n",
      "features.7.0.block.2.fc1.bias\n",
      "features.7.0.block.2.fc2.weight\n",
      "features.7.0.block.2.fc2.bias\n",
      "features.7.0.block.3.0.weight\n",
      "features.7.0.block.3.1.weight\n",
      "features.7.0.block.3.1.bias\n",
      "features.7.0.block.3.1.running_mean\n",
      "features.7.0.block.3.1.running_var\n",
      "features.7.0.block.3.1.num_batches_tracked\n",
      "features.8.0.weight\n",
      "features.8.1.weight\n",
      "features.8.1.bias\n",
      "features.8.1.running_mean\n",
      "features.8.1.running_var\n",
      "features.8.1.num_batches_tracked\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "# 初始化模型\n",
    "model = efficientnet_b0(pretrained=False)  # 不使用在线预训练模型，手动加载\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# 打印状态字典中的所有键\n",
    "for key in model_dict.keys():\n",
    "    print(key)\n",
    "# 加载本地保存的权重\n",
    "\n",
    "# 将模型移到GPU（如果需要）\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "features.0.0.weight\n",
      "features.0.1.weight\n",
      "features.0.1.bias\n",
      "features.0.1.running_mean\n",
      "features.0.1.running_var\n",
      "features.0.1.num_batches_tracked\n",
      "features.1.0.block.0.0.weight\n",
      "features.1.0.block.0.1.weight\n",
      "features.1.0.block.0.1.bias\n",
      "features.1.0.block.0.1.running_mean\n",
      "features.1.0.block.0.1.running_var\n",
      "features.1.0.block.0.1.num_batches_tracked\n",
      "features.1.0.block.1.fc1.weight\n",
      "features.1.0.block.1.fc1.bias\n",
      "features.1.0.block.1.fc2.weight\n",
      "features.1.0.block.1.fc2.bias\n",
      "features.1.0.block.2.0.weight\n",
      "features.1.0.block.2.1.weight\n",
      "features.1.0.block.2.1.bias\n",
      "features.1.0.block.2.1.running_mean\n",
      "features.1.0.block.2.1.running_var\n",
      "features.1.0.block.2.1.num_batches_tracked\n",
      "features.2.0.block.0.0.weight\n",
      "features.2.0.block.0.1.weight\n",
      "features.2.0.block.0.1.bias\n",
      "features.2.0.block.0.1.running_mean\n",
      "features.2.0.block.0.1.running_var\n",
      "features.2.0.block.0.1.num_batches_tracked\n",
      "features.2.0.block.1.0.weight\n",
      "features.2.0.block.1.1.weight\n",
      "features.2.0.block.1.1.bias\n",
      "features.2.0.block.1.1.running_mean\n",
      "features.2.0.block.1.1.running_var\n",
      "features.2.0.block.1.1.num_batches_tracked\n",
      "features.2.0.block.2.fc1.weight\n",
      "features.2.0.block.2.fc1.bias\n",
      "features.2.0.block.2.fc2.weight\n",
      "features.2.0.block.2.fc2.bias\n",
      "features.2.0.block.3.0.weight\n",
      "features.2.0.block.3.1.weight\n",
      "features.2.0.block.3.1.bias\n",
      "features.2.0.block.3.1.running_mean\n",
      "features.2.0.block.3.1.running_var\n",
      "features.2.0.block.3.1.num_batches_tracked\n",
      "features.2.1.block.0.0.weight\n",
      "features.2.1.block.0.1.weight\n",
      "features.2.1.block.0.1.bias\n",
      "features.2.1.block.0.1.running_mean\n",
      "features.2.1.block.0.1.running_var\n",
      "features.2.1.block.0.1.num_batches_tracked\n",
      "features.2.1.block.1.0.weight\n",
      "features.2.1.block.1.1.weight\n",
      "features.2.1.block.1.1.bias\n",
      "features.2.1.block.1.1.running_mean\n",
      "features.2.1.block.1.1.running_var\n",
      "features.2.1.block.1.1.num_batches_tracked\n",
      "features.2.1.block.2.fc1.weight\n",
      "features.2.1.block.2.fc1.bias\n",
      "features.2.1.block.2.fc2.weight\n",
      "features.2.1.block.2.fc2.bias\n",
      "features.2.1.block.3.0.weight\n",
      "features.2.1.block.3.1.weight\n",
      "features.2.1.block.3.1.bias\n",
      "features.2.1.block.3.1.running_mean\n",
      "features.2.1.block.3.1.running_var\n",
      "features.2.1.block.3.1.num_batches_tracked\n",
      "features.3.0.block.0.0.weight\n",
      "features.3.0.block.0.1.weight\n",
      "features.3.0.block.0.1.bias\n",
      "features.3.0.block.0.1.running_mean\n",
      "features.3.0.block.0.1.running_var\n",
      "features.3.0.block.0.1.num_batches_tracked\n",
      "features.3.0.block.1.0.weight\n",
      "features.3.0.block.1.1.weight\n",
      "features.3.0.block.1.1.bias\n",
      "features.3.0.block.1.1.running_mean\n",
      "features.3.0.block.1.1.running_var\n",
      "features.3.0.block.1.1.num_batches_tracked\n",
      "features.3.0.block.2.fc1.weight\n",
      "features.3.0.block.2.fc1.bias\n",
      "features.3.0.block.2.fc2.weight\n",
      "features.3.0.block.2.fc2.bias\n",
      "features.3.0.block.3.0.weight\n",
      "features.3.0.block.3.1.weight\n",
      "features.3.0.block.3.1.bias\n",
      "features.3.0.block.3.1.running_mean\n",
      "features.3.0.block.3.1.running_var\n",
      "features.3.0.block.3.1.num_batches_tracked\n",
      "features.3.1.block.0.0.weight\n",
      "features.3.1.block.0.1.weight\n",
      "features.3.1.block.0.1.bias\n",
      "features.3.1.block.0.1.running_mean\n",
      "features.3.1.block.0.1.running_var\n",
      "features.3.1.block.0.1.num_batches_tracked\n",
      "features.3.1.block.1.0.weight\n",
      "features.3.1.block.1.1.weight\n",
      "features.3.1.block.1.1.bias\n",
      "features.3.1.block.1.1.running_mean\n",
      "features.3.1.block.1.1.running_var\n",
      "features.3.1.block.1.1.num_batches_tracked\n",
      "features.3.1.block.2.fc1.weight\n",
      "features.3.1.block.2.fc1.bias\n",
      "features.3.1.block.2.fc2.weight\n",
      "features.3.1.block.2.fc2.bias\n",
      "features.3.1.block.3.0.weight\n",
      "features.3.1.block.3.1.weight\n",
      "features.3.1.block.3.1.bias\n",
      "features.3.1.block.3.1.running_mean\n",
      "features.3.1.block.3.1.running_var\n",
      "features.3.1.block.3.1.num_batches_tracked\n",
      "features.4.0.block.0.0.weight\n",
      "features.4.0.block.0.1.weight\n",
      "features.4.0.block.0.1.bias\n",
      "features.4.0.block.0.1.running_mean\n",
      "features.4.0.block.0.1.running_var\n",
      "features.4.0.block.0.1.num_batches_tracked\n",
      "features.4.0.block.1.0.weight\n",
      "features.4.0.block.1.1.weight\n",
      "features.4.0.block.1.1.bias\n",
      "features.4.0.block.1.1.running_mean\n",
      "features.4.0.block.1.1.running_var\n",
      "features.4.0.block.1.1.num_batches_tracked\n",
      "features.4.0.block.2.fc1.weight\n",
      "features.4.0.block.2.fc1.bias\n",
      "features.4.0.block.2.fc2.weight\n",
      "features.4.0.block.2.fc2.bias\n",
      "features.4.0.block.3.0.weight\n",
      "features.4.0.block.3.1.weight\n",
      "features.4.0.block.3.1.bias\n",
      "features.4.0.block.3.1.running_mean\n",
      "features.4.0.block.3.1.running_var\n",
      "features.4.0.block.3.1.num_batches_tracked\n",
      "features.4.1.block.0.0.weight\n",
      "features.4.1.block.0.1.weight\n",
      "features.4.1.block.0.1.bias\n",
      "features.4.1.block.0.1.running_mean\n",
      "features.4.1.block.0.1.running_var\n",
      "features.4.1.block.0.1.num_batches_tracked\n",
      "features.4.1.block.1.0.weight\n",
      "features.4.1.block.1.1.weight\n",
      "features.4.1.block.1.1.bias\n",
      "features.4.1.block.1.1.running_mean\n",
      "features.4.1.block.1.1.running_var\n",
      "features.4.1.block.1.1.num_batches_tracked\n",
      "features.4.1.block.2.fc1.weight\n",
      "features.4.1.block.2.fc1.bias\n",
      "features.4.1.block.2.fc2.weight\n",
      "features.4.1.block.2.fc2.bias\n",
      "features.4.1.block.3.0.weight\n",
      "features.4.1.block.3.1.weight\n",
      "features.4.1.block.3.1.bias\n",
      "features.4.1.block.3.1.running_mean\n",
      "features.4.1.block.3.1.running_var\n",
      "features.4.1.block.3.1.num_batches_tracked\n",
      "features.4.2.block.0.0.weight\n",
      "features.4.2.block.0.1.weight\n",
      "features.4.2.block.0.1.bias\n",
      "features.4.2.block.0.1.running_mean\n",
      "features.4.2.block.0.1.running_var\n",
      "features.4.2.block.0.1.num_batches_tracked\n",
      "features.4.2.block.1.0.weight\n",
      "features.4.2.block.1.1.weight\n",
      "features.4.2.block.1.1.bias\n",
      "features.4.2.block.1.1.running_mean\n",
      "features.4.2.block.1.1.running_var\n",
      "features.4.2.block.1.1.num_batches_tracked\n",
      "features.4.2.block.2.fc1.weight\n",
      "features.4.2.block.2.fc1.bias\n",
      "features.4.2.block.2.fc2.weight\n",
      "features.4.2.block.2.fc2.bias\n",
      "features.4.2.block.3.0.weight\n",
      "features.4.2.block.3.1.weight\n",
      "features.4.2.block.3.1.bias\n",
      "features.4.2.block.3.1.running_mean\n",
      "features.4.2.block.3.1.running_var\n",
      "features.4.2.block.3.1.num_batches_tracked\n",
      "features.5.0.block.0.0.weight\n",
      "features.5.0.block.0.1.weight\n",
      "features.5.0.block.0.1.bias\n",
      "features.5.0.block.0.1.running_mean\n",
      "features.5.0.block.0.1.running_var\n",
      "features.5.0.block.0.1.num_batches_tracked\n",
      "features.5.0.block.1.0.weight\n",
      "features.5.0.block.1.1.weight\n",
      "features.5.0.block.1.1.bias\n",
      "features.5.0.block.1.1.running_mean\n",
      "features.5.0.block.1.1.running_var\n",
      "features.5.0.block.1.1.num_batches_tracked\n",
      "features.5.0.block.2.fc1.weight\n",
      "features.5.0.block.2.fc1.bias\n",
      "features.5.0.block.2.fc2.weight\n",
      "features.5.0.block.2.fc2.bias\n",
      "features.5.0.block.3.0.weight\n",
      "features.5.0.block.3.1.weight\n",
      "features.5.0.block.3.1.bias\n",
      "features.5.0.block.3.1.running_mean\n",
      "features.5.0.block.3.1.running_var\n",
      "features.5.0.block.3.1.num_batches_tracked\n",
      "features.5.1.block.0.0.weight\n",
      "features.5.1.block.0.1.weight\n",
      "features.5.1.block.0.1.bias\n",
      "features.5.1.block.0.1.running_mean\n",
      "features.5.1.block.0.1.running_var\n",
      "features.5.1.block.0.1.num_batches_tracked\n",
      "features.5.1.block.1.0.weight\n",
      "features.5.1.block.1.1.weight\n",
      "features.5.1.block.1.1.bias\n",
      "features.5.1.block.1.1.running_mean\n",
      "features.5.1.block.1.1.running_var\n",
      "features.5.1.block.1.1.num_batches_tracked\n",
      "features.5.1.block.2.fc1.weight\n",
      "features.5.1.block.2.fc1.bias\n",
      "features.5.1.block.2.fc2.weight\n",
      "features.5.1.block.2.fc2.bias\n",
      "features.5.1.block.3.0.weight\n",
      "features.5.1.block.3.1.weight\n",
      "features.5.1.block.3.1.bias\n",
      "features.5.1.block.3.1.running_mean\n",
      "features.5.1.block.3.1.running_var\n",
      "features.5.1.block.3.1.num_batches_tracked\n",
      "features.5.2.block.0.0.weight\n",
      "features.5.2.block.0.1.weight\n",
      "features.5.2.block.0.1.bias\n",
      "features.5.2.block.0.1.running_mean\n",
      "features.5.2.block.0.1.running_var\n",
      "features.5.2.block.0.1.num_batches_tracked\n",
      "features.5.2.block.1.0.weight\n",
      "features.5.2.block.1.1.weight\n",
      "features.5.2.block.1.1.bias\n",
      "features.5.2.block.1.1.running_mean\n",
      "features.5.2.block.1.1.running_var\n",
      "features.5.2.block.1.1.num_batches_tracked\n",
      "features.5.2.block.2.fc1.weight\n",
      "features.5.2.block.2.fc1.bias\n",
      "features.5.2.block.2.fc2.weight\n",
      "features.5.2.block.2.fc2.bias\n",
      "features.5.2.block.3.0.weight\n",
      "features.5.2.block.3.1.weight\n",
      "features.5.2.block.3.1.bias\n",
      "features.5.2.block.3.1.running_mean\n",
      "features.5.2.block.3.1.running_var\n",
      "features.5.2.block.3.1.num_batches_tracked\n",
      "features.6.0.block.0.0.weight\n",
      "features.6.0.block.0.1.weight\n",
      "features.6.0.block.0.1.bias\n",
      "features.6.0.block.0.1.running_mean\n",
      "features.6.0.block.0.1.running_var\n",
      "features.6.0.block.0.1.num_batches_tracked\n",
      "features.6.0.block.1.0.weight\n",
      "features.6.0.block.1.1.weight\n",
      "features.6.0.block.1.1.bias\n",
      "features.6.0.block.1.1.running_mean\n",
      "features.6.0.block.1.1.running_var\n",
      "features.6.0.block.1.1.num_batches_tracked\n",
      "features.6.0.block.2.fc1.weight\n",
      "features.6.0.block.2.fc1.bias\n",
      "features.6.0.block.2.fc2.weight\n",
      "features.6.0.block.2.fc2.bias\n",
      "features.6.0.block.3.0.weight\n",
      "features.6.0.block.3.1.weight\n",
      "features.6.0.block.3.1.bias\n",
      "features.6.0.block.3.1.running_mean\n",
      "features.6.0.block.3.1.running_var\n",
      "features.6.0.block.3.1.num_batches_tracked\n",
      "features.6.1.block.0.0.weight\n",
      "features.6.1.block.0.1.weight\n",
      "features.6.1.block.0.1.bias\n",
      "features.6.1.block.0.1.running_mean\n",
      "features.6.1.block.0.1.running_var\n",
      "features.6.1.block.0.1.num_batches_tracked\n",
      "features.6.1.block.1.0.weight\n",
      "features.6.1.block.1.1.weight\n",
      "features.6.1.block.1.1.bias\n",
      "features.6.1.block.1.1.running_mean\n",
      "features.6.1.block.1.1.running_var\n",
      "features.6.1.block.1.1.num_batches_tracked\n",
      "features.6.1.block.2.fc1.weight\n",
      "features.6.1.block.2.fc1.bias\n",
      "features.6.1.block.2.fc2.weight\n",
      "features.6.1.block.2.fc2.bias\n",
      "features.6.1.block.3.0.weight\n",
      "features.6.1.block.3.1.weight\n",
      "features.6.1.block.3.1.bias\n",
      "features.6.1.block.3.1.running_mean\n",
      "features.6.1.block.3.1.running_var\n",
      "features.6.1.block.3.1.num_batches_tracked\n",
      "features.6.2.block.0.0.weight\n",
      "features.6.2.block.0.1.weight\n",
      "features.6.2.block.0.1.bias\n",
      "features.6.2.block.0.1.running_mean\n",
      "features.6.2.block.0.1.running_var\n",
      "features.6.2.block.0.1.num_batches_tracked\n",
      "features.6.2.block.1.0.weight\n",
      "features.6.2.block.1.1.weight\n",
      "features.6.2.block.1.1.bias\n",
      "features.6.2.block.1.1.running_mean\n",
      "features.6.2.block.1.1.running_var\n",
      "features.6.2.block.1.1.num_batches_tracked\n",
      "features.6.2.block.2.fc1.weight\n",
      "features.6.2.block.2.fc1.bias\n",
      "features.6.2.block.2.fc2.weight\n",
      "features.6.2.block.2.fc2.bias\n",
      "features.6.2.block.3.0.weight\n",
      "features.6.2.block.3.1.weight\n",
      "features.6.2.block.3.1.bias\n",
      "features.6.2.block.3.1.running_mean\n",
      "features.6.2.block.3.1.running_var\n",
      "features.6.2.block.3.1.num_batches_tracked\n",
      "features.6.3.block.0.0.weight\n",
      "features.6.3.block.0.1.weight\n",
      "features.6.3.block.0.1.bias\n",
      "features.6.3.block.0.1.running_mean\n",
      "features.6.3.block.0.1.running_var\n",
      "features.6.3.block.0.1.num_batches_tracked\n",
      "features.6.3.block.1.0.weight\n",
      "features.6.3.block.1.1.weight\n",
      "features.6.3.block.1.1.bias\n",
      "features.6.3.block.1.1.running_mean\n",
      "features.6.3.block.1.1.running_var\n",
      "features.6.3.block.1.1.num_batches_tracked\n",
      "features.6.3.block.2.fc1.weight\n",
      "features.6.3.block.2.fc1.bias\n",
      "features.6.3.block.2.fc2.weight\n",
      "features.6.3.block.2.fc2.bias\n",
      "features.6.3.block.3.0.weight\n",
      "features.6.3.block.3.1.weight\n",
      "features.6.3.block.3.1.bias\n",
      "features.6.3.block.3.1.running_mean\n",
      "features.6.3.block.3.1.running_var\n",
      "features.6.3.block.3.1.num_batches_tracked\n",
      "features.7.0.block.0.0.weight\n",
      "features.7.0.block.0.1.weight\n",
      "features.7.0.block.0.1.bias\n",
      "features.7.0.block.0.1.running_mean\n",
      "features.7.0.block.0.1.running_var\n",
      "features.7.0.block.0.1.num_batches_tracked\n",
      "features.7.0.block.1.0.weight\n",
      "features.7.0.block.1.1.weight\n",
      "features.7.0.block.1.1.bias\n",
      "features.7.0.block.1.1.running_mean\n",
      "features.7.0.block.1.1.running_var\n",
      "features.7.0.block.1.1.num_batches_tracked\n",
      "features.7.0.block.2.fc1.weight\n",
      "features.7.0.block.2.fc1.bias\n",
      "features.7.0.block.2.fc2.weight\n",
      "features.7.0.block.2.fc2.bias\n",
      "features.7.0.block.3.0.weight\n",
      "features.7.0.block.3.1.weight\n",
      "features.7.0.block.3.1.bias\n",
      "features.7.0.block.3.1.running_mean\n",
      "features.7.0.block.3.1.running_var\n",
      "features.7.0.block.3.1.num_batches_tracked\n",
      "features.8.0.weight\n",
      "features.8.1.weight\n",
      "features.8.1.bias\n",
      "features.8.1.running_mean\n",
      "features.8.1.running_var\n",
      "features.8.1.num_batches_tracked\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../efficientnet_b0.pth'))\n",
    "\n",
    "print(\"\\n\")\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# 打印状态字典中的所有键\n",
    "for key in model_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "_conv_stem.weight\n",
      "_bn0.weight\n",
      "_bn0.bias\n",
      "_bn0.running_mean\n",
      "_bn0.running_var\n",
      "_bn0.num_batches_tracked\n",
      "_blocks.0._depthwise_conv.weight\n",
      "_blocks.0._bn1.weight\n",
      "_blocks.0._bn1.bias\n",
      "_blocks.0._bn1.running_mean\n",
      "_blocks.0._bn1.running_var\n",
      "_blocks.0._bn1.num_batches_tracked\n",
      "_blocks.0._se_reduce.weight\n",
      "_blocks.0._se_reduce.bias\n",
      "_blocks.0._se_expand.weight\n",
      "_blocks.0._se_expand.bias\n",
      "_blocks.0._project_conv.weight\n",
      "_blocks.0._bn2.weight\n",
      "_blocks.0._bn2.bias\n",
      "_blocks.0._bn2.running_mean\n",
      "_blocks.0._bn2.running_var\n",
      "_blocks.0._bn2.num_batches_tracked\n",
      "_blocks.1._expand_conv.weight\n",
      "_blocks.1._bn0.weight\n",
      "_blocks.1._bn0.bias\n",
      "_blocks.1._bn0.running_mean\n",
      "_blocks.1._bn0.running_var\n",
      "_blocks.1._bn0.num_batches_tracked\n",
      "_blocks.1._depthwise_conv.weight\n",
      "_blocks.1._bn1.weight\n",
      "_blocks.1._bn1.bias\n",
      "_blocks.1._bn1.running_mean\n",
      "_blocks.1._bn1.running_var\n",
      "_blocks.1._bn1.num_batches_tracked\n",
      "_blocks.1._se_reduce.weight\n",
      "_blocks.1._se_reduce.bias\n",
      "_blocks.1._se_expand.weight\n",
      "_blocks.1._se_expand.bias\n",
      "_blocks.1._project_conv.weight\n",
      "_blocks.1._bn2.weight\n",
      "_blocks.1._bn2.bias\n",
      "_blocks.1._bn2.running_mean\n",
      "_blocks.1._bn2.running_var\n",
      "_blocks.1._bn2.num_batches_tracked\n",
      "_blocks.2._expand_conv.weight\n",
      "_blocks.2._bn0.weight\n",
      "_blocks.2._bn0.bias\n",
      "_blocks.2._bn0.running_mean\n",
      "_blocks.2._bn0.running_var\n",
      "_blocks.2._bn0.num_batches_tracked\n",
      "_blocks.2._depthwise_conv.weight\n",
      "_blocks.2._bn1.weight\n",
      "_blocks.2._bn1.bias\n",
      "_blocks.2._bn1.running_mean\n",
      "_blocks.2._bn1.running_var\n",
      "_blocks.2._bn1.num_batches_tracked\n",
      "_blocks.2._se_reduce.weight\n",
      "_blocks.2._se_reduce.bias\n",
      "_blocks.2._se_expand.weight\n",
      "_blocks.2._se_expand.bias\n",
      "_blocks.2._project_conv.weight\n",
      "_blocks.2._bn2.weight\n",
      "_blocks.2._bn2.bias\n",
      "_blocks.2._bn2.running_mean\n",
      "_blocks.2._bn2.running_var\n",
      "_blocks.2._bn2.num_batches_tracked\n",
      "_blocks.3._expand_conv.weight\n",
      "_blocks.3._bn0.weight\n",
      "_blocks.3._bn0.bias\n",
      "_blocks.3._bn0.running_mean\n",
      "_blocks.3._bn0.running_var\n",
      "_blocks.3._bn0.num_batches_tracked\n",
      "_blocks.3._depthwise_conv.weight\n",
      "_blocks.3._bn1.weight\n",
      "_blocks.3._bn1.bias\n",
      "_blocks.3._bn1.running_mean\n",
      "_blocks.3._bn1.running_var\n",
      "_blocks.3._bn1.num_batches_tracked\n",
      "_blocks.3._se_reduce.weight\n",
      "_blocks.3._se_reduce.bias\n",
      "_blocks.3._se_expand.weight\n",
      "_blocks.3._se_expand.bias\n",
      "_blocks.3._project_conv.weight\n",
      "_blocks.3._bn2.weight\n",
      "_blocks.3._bn2.bias\n",
      "_blocks.3._bn2.running_mean\n",
      "_blocks.3._bn2.running_var\n",
      "_blocks.3._bn2.num_batches_tracked\n",
      "_blocks.4._expand_conv.weight\n",
      "_blocks.4._bn0.weight\n",
      "_blocks.4._bn0.bias\n",
      "_blocks.4._bn0.running_mean\n",
      "_blocks.4._bn0.running_var\n",
      "_blocks.4._bn0.num_batches_tracked\n",
      "_blocks.4._depthwise_conv.weight\n",
      "_blocks.4._bn1.weight\n",
      "_blocks.4._bn1.bias\n",
      "_blocks.4._bn1.running_mean\n",
      "_blocks.4._bn1.running_var\n",
      "_blocks.4._bn1.num_batches_tracked\n",
      "_blocks.4._se_reduce.weight\n",
      "_blocks.4._se_reduce.bias\n",
      "_blocks.4._se_expand.weight\n",
      "_blocks.4._se_expand.bias\n",
      "_blocks.4._project_conv.weight\n",
      "_blocks.4._bn2.weight\n",
      "_blocks.4._bn2.bias\n",
      "_blocks.4._bn2.running_mean\n",
      "_blocks.4._bn2.running_var\n",
      "_blocks.4._bn2.num_batches_tracked\n",
      "_blocks.5._expand_conv.weight\n",
      "_blocks.5._bn0.weight\n",
      "_blocks.5._bn0.bias\n",
      "_blocks.5._bn0.running_mean\n",
      "_blocks.5._bn0.running_var\n",
      "_blocks.5._bn0.num_batches_tracked\n",
      "_blocks.5._depthwise_conv.weight\n",
      "_blocks.5._bn1.weight\n",
      "_blocks.5._bn1.bias\n",
      "_blocks.5._bn1.running_mean\n",
      "_blocks.5._bn1.running_var\n",
      "_blocks.5._bn1.num_batches_tracked\n",
      "_blocks.5._se_reduce.weight\n",
      "_blocks.5._se_reduce.bias\n",
      "_blocks.5._se_expand.weight\n",
      "_blocks.5._se_expand.bias\n",
      "_blocks.5._project_conv.weight\n",
      "_blocks.5._bn2.weight\n",
      "_blocks.5._bn2.bias\n",
      "_blocks.5._bn2.running_mean\n",
      "_blocks.5._bn2.running_var\n",
      "_blocks.5._bn2.num_batches_tracked\n",
      "_blocks.6._expand_conv.weight\n",
      "_blocks.6._bn0.weight\n",
      "_blocks.6._bn0.bias\n",
      "_blocks.6._bn0.running_mean\n",
      "_blocks.6._bn0.running_var\n",
      "_blocks.6._bn0.num_batches_tracked\n",
      "_blocks.6._depthwise_conv.weight\n",
      "_blocks.6._bn1.weight\n",
      "_blocks.6._bn1.bias\n",
      "_blocks.6._bn1.running_mean\n",
      "_blocks.6._bn1.running_var\n",
      "_blocks.6._bn1.num_batches_tracked\n",
      "_blocks.6._se_reduce.weight\n",
      "_blocks.6._se_reduce.bias\n",
      "_blocks.6._se_expand.weight\n",
      "_blocks.6._se_expand.bias\n",
      "_blocks.6._project_conv.weight\n",
      "_blocks.6._bn2.weight\n",
      "_blocks.6._bn2.bias\n",
      "_blocks.6._bn2.running_mean\n",
      "_blocks.6._bn2.running_var\n",
      "_blocks.6._bn2.num_batches_tracked\n",
      "_blocks.7._expand_conv.weight\n",
      "_blocks.7._bn0.weight\n",
      "_blocks.7._bn0.bias\n",
      "_blocks.7._bn0.running_mean\n",
      "_blocks.7._bn0.running_var\n",
      "_blocks.7._bn0.num_batches_tracked\n",
      "_blocks.7._depthwise_conv.weight\n",
      "_blocks.7._bn1.weight\n",
      "_blocks.7._bn1.bias\n",
      "_blocks.7._bn1.running_mean\n",
      "_blocks.7._bn1.running_var\n",
      "_blocks.7._bn1.num_batches_tracked\n",
      "_blocks.7._se_reduce.weight\n",
      "_blocks.7._se_reduce.bias\n",
      "_blocks.7._se_expand.weight\n",
      "_blocks.7._se_expand.bias\n",
      "_blocks.7._project_conv.weight\n",
      "_blocks.7._bn2.weight\n",
      "_blocks.7._bn2.bias\n",
      "_blocks.7._bn2.running_mean\n",
      "_blocks.7._bn2.running_var\n",
      "_blocks.7._bn2.num_batches_tracked\n",
      "_blocks.8._expand_conv.weight\n",
      "_blocks.8._bn0.weight\n",
      "_blocks.8._bn0.bias\n",
      "_blocks.8._bn0.running_mean\n",
      "_blocks.8._bn0.running_var\n",
      "_blocks.8._bn0.num_batches_tracked\n",
      "_blocks.8._depthwise_conv.weight\n",
      "_blocks.8._bn1.weight\n",
      "_blocks.8._bn1.bias\n",
      "_blocks.8._bn1.running_mean\n",
      "_blocks.8._bn1.running_var\n",
      "_blocks.8._bn1.num_batches_tracked\n",
      "_blocks.8._se_reduce.weight\n",
      "_blocks.8._se_reduce.bias\n",
      "_blocks.8._se_expand.weight\n",
      "_blocks.8._se_expand.bias\n",
      "_blocks.8._project_conv.weight\n",
      "_blocks.8._bn2.weight\n",
      "_blocks.8._bn2.bias\n",
      "_blocks.8._bn2.running_mean\n",
      "_blocks.8._bn2.running_var\n",
      "_blocks.8._bn2.num_batches_tracked\n",
      "_blocks.9._expand_conv.weight\n",
      "_blocks.9._bn0.weight\n",
      "_blocks.9._bn0.bias\n",
      "_blocks.9._bn0.running_mean\n",
      "_blocks.9._bn0.running_var\n",
      "_blocks.9._bn0.num_batches_tracked\n",
      "_blocks.9._depthwise_conv.weight\n",
      "_blocks.9._bn1.weight\n",
      "_blocks.9._bn1.bias\n",
      "_blocks.9._bn1.running_mean\n",
      "_blocks.9._bn1.running_var\n",
      "_blocks.9._bn1.num_batches_tracked\n",
      "_blocks.9._se_reduce.weight\n",
      "_blocks.9._se_reduce.bias\n",
      "_blocks.9._se_expand.weight\n",
      "_blocks.9._se_expand.bias\n",
      "_blocks.9._project_conv.weight\n",
      "_blocks.9._bn2.weight\n",
      "_blocks.9._bn2.bias\n",
      "_blocks.9._bn2.running_mean\n",
      "_blocks.9._bn2.running_var\n",
      "_blocks.9._bn2.num_batches_tracked\n",
      "_blocks.10._expand_conv.weight\n",
      "_blocks.10._bn0.weight\n",
      "_blocks.10._bn0.bias\n",
      "_blocks.10._bn0.running_mean\n",
      "_blocks.10._bn0.running_var\n",
      "_blocks.10._bn0.num_batches_tracked\n",
      "_blocks.10._depthwise_conv.weight\n",
      "_blocks.10._bn1.weight\n",
      "_blocks.10._bn1.bias\n",
      "_blocks.10._bn1.running_mean\n",
      "_blocks.10._bn1.running_var\n",
      "_blocks.10._bn1.num_batches_tracked\n",
      "_blocks.10._se_reduce.weight\n",
      "_blocks.10._se_reduce.bias\n",
      "_blocks.10._se_expand.weight\n",
      "_blocks.10._se_expand.bias\n",
      "_blocks.10._project_conv.weight\n",
      "_blocks.10._bn2.weight\n",
      "_blocks.10._bn2.bias\n",
      "_blocks.10._bn2.running_mean\n",
      "_blocks.10._bn2.running_var\n",
      "_blocks.10._bn2.num_batches_tracked\n",
      "_blocks.11._expand_conv.weight\n",
      "_blocks.11._bn0.weight\n",
      "_blocks.11._bn0.bias\n",
      "_blocks.11._bn0.running_mean\n",
      "_blocks.11._bn0.running_var\n",
      "_blocks.11._bn0.num_batches_tracked\n",
      "_blocks.11._depthwise_conv.weight\n",
      "_blocks.11._bn1.weight\n",
      "_blocks.11._bn1.bias\n",
      "_blocks.11._bn1.running_mean\n",
      "_blocks.11._bn1.running_var\n",
      "_blocks.11._bn1.num_batches_tracked\n",
      "_blocks.11._se_reduce.weight\n",
      "_blocks.11._se_reduce.bias\n",
      "_blocks.11._se_expand.weight\n",
      "_blocks.11._se_expand.bias\n",
      "_blocks.11._project_conv.weight\n",
      "_blocks.11._bn2.weight\n",
      "_blocks.11._bn2.bias\n",
      "_blocks.11._bn2.running_mean\n",
      "_blocks.11._bn2.running_var\n",
      "_blocks.11._bn2.num_batches_tracked\n",
      "_blocks.12._expand_conv.weight\n",
      "_blocks.12._bn0.weight\n",
      "_blocks.12._bn0.bias\n",
      "_blocks.12._bn0.running_mean\n",
      "_blocks.12._bn0.running_var\n",
      "_blocks.12._bn0.num_batches_tracked\n",
      "_blocks.12._depthwise_conv.weight\n",
      "_blocks.12._bn1.weight\n",
      "_blocks.12._bn1.bias\n",
      "_blocks.12._bn1.running_mean\n",
      "_blocks.12._bn1.running_var\n",
      "_blocks.12._bn1.num_batches_tracked\n",
      "_blocks.12._se_reduce.weight\n",
      "_blocks.12._se_reduce.bias\n",
      "_blocks.12._se_expand.weight\n",
      "_blocks.12._se_expand.bias\n",
      "_blocks.12._project_conv.weight\n",
      "_blocks.12._bn2.weight\n",
      "_blocks.12._bn2.bias\n",
      "_blocks.12._bn2.running_mean\n",
      "_blocks.12._bn2.running_var\n",
      "_blocks.12._bn2.num_batches_tracked\n",
      "_blocks.13._expand_conv.weight\n",
      "_blocks.13._bn0.weight\n",
      "_blocks.13._bn0.bias\n",
      "_blocks.13._bn0.running_mean\n",
      "_blocks.13._bn0.running_var\n",
      "_blocks.13._bn0.num_batches_tracked\n",
      "_blocks.13._depthwise_conv.weight\n",
      "_blocks.13._bn1.weight\n",
      "_blocks.13._bn1.bias\n",
      "_blocks.13._bn1.running_mean\n",
      "_blocks.13._bn1.running_var\n",
      "_blocks.13._bn1.num_batches_tracked\n",
      "_blocks.13._se_reduce.weight\n",
      "_blocks.13._se_reduce.bias\n",
      "_blocks.13._se_expand.weight\n",
      "_blocks.13._se_expand.bias\n",
      "_blocks.13._project_conv.weight\n",
      "_blocks.13._bn2.weight\n",
      "_blocks.13._bn2.bias\n",
      "_blocks.13._bn2.running_mean\n",
      "_blocks.13._bn2.running_var\n",
      "_blocks.13._bn2.num_batches_tracked\n",
      "_blocks.14._expand_conv.weight\n",
      "_blocks.14._bn0.weight\n",
      "_blocks.14._bn0.bias\n",
      "_blocks.14._bn0.running_mean\n",
      "_blocks.14._bn0.running_var\n",
      "_blocks.14._bn0.num_batches_tracked\n",
      "_blocks.14._depthwise_conv.weight\n",
      "_blocks.14._bn1.weight\n",
      "_blocks.14._bn1.bias\n",
      "_blocks.14._bn1.running_mean\n",
      "_blocks.14._bn1.running_var\n",
      "_blocks.14._bn1.num_batches_tracked\n",
      "_blocks.14._se_reduce.weight\n",
      "_blocks.14._se_reduce.bias\n",
      "_blocks.14._se_expand.weight\n",
      "_blocks.14._se_expand.bias\n",
      "_blocks.14._project_conv.weight\n",
      "_blocks.14._bn2.weight\n",
      "_blocks.14._bn2.bias\n",
      "_blocks.14._bn2.running_mean\n",
      "_blocks.14._bn2.running_var\n",
      "_blocks.14._bn2.num_batches_tracked\n",
      "_blocks.15._expand_conv.weight\n",
      "_blocks.15._bn0.weight\n",
      "_blocks.15._bn0.bias\n",
      "_blocks.15._bn0.running_mean\n",
      "_blocks.15._bn0.running_var\n",
      "_blocks.15._bn0.num_batches_tracked\n",
      "_blocks.15._depthwise_conv.weight\n",
      "_blocks.15._bn1.weight\n",
      "_blocks.15._bn1.bias\n",
      "_blocks.15._bn1.running_mean\n",
      "_blocks.15._bn1.running_var\n",
      "_blocks.15._bn1.num_batches_tracked\n",
      "_blocks.15._se_reduce.weight\n",
      "_blocks.15._se_reduce.bias\n",
      "_blocks.15._se_expand.weight\n",
      "_blocks.15._se_expand.bias\n",
      "_blocks.15._project_conv.weight\n",
      "_blocks.15._bn2.weight\n",
      "_blocks.15._bn2.bias\n",
      "_blocks.15._bn2.running_mean\n",
      "_blocks.15._bn2.running_var\n",
      "_blocks.15._bn2.num_batches_tracked\n",
      "_conv_head.weight\n",
      "_bn1.weight\n",
      "_bn1.bias\n",
      "_bn1.running_mean\n",
      "_bn1.running_var\n",
      "_bn1.num_batches_tracked\n",
      "_fc.weight\n",
      "_fc.bias\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "model_dict = model.state_dict()\n",
    "for key in model_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runhui/miniconda3/envs/skin/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/runhui/miniconda3/envs/skin/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /home/runhui/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
      "100%|██████████| 255M/255M [00:04<00:00, 58.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import efficientnet_b0, efficientnet_b7\n",
    "\n",
    "# 下载预训练的EfficientNet-B0模型\n",
    "model = efficientnet_b7(pretrained=True)\n",
    "\n",
    "# 保存模型权重到本地文件\n",
    "torch.save(model.state_dict(), 'efficientnet_b7.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
